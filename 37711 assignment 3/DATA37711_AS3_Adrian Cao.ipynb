{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Causal DAGs\n",
    "\n",
    "## (a) Negative association between COVID-19 severity and smoking\n",
    "\n",
    "- **Confounder**:  A confounder (e.g., age, health status, or other underlying conditions) influences both smoking and the severity of COVID-19. For example, smoking might correlate with age, and younger individuals are more likely to smoke also be at lower risk for severe COVID-19 infections. Thus, **age acts as a confounder**, leading to the observed negative association.\n",
    "\n",
    "    Causal DAG:\n",
    "    ```\n",
    "    Smoking <-- Age --> COVID-19 Severity\n",
    "    ```\n",
    "\n",
    "  Without controlling for age, the observed association between smoking and COVID-19 severity may not reflect a direct causal relationship.\n",
    "\n",
    "- **Collider Bias**: If severity of COVID-19 and smoking both influence hospitalization or testing rates (e.g., only severe cases or smokers are hospitalized/tested more often), selection bias (conditioning on hospitalization) could create a spurious association.\n",
    "\n",
    "    Causal DAG with collider:\n",
    "    ```\n",
    "    Smoking --> Hospitalization <-- COVID-19 Severity\n",
    "    ```\n",
    "\n",
    "    Conditioning on hospitalization (a collider) might create an artificial negative association.\n",
    "\n",
    "## (b) Simpson’s Paradox in Berkeley Admissions\n",
    "This is a **classic example of Simpson’s Paradox**. The overall acceptance rates can differ between men and women due to **confounding by department**. For example, Male applicants may disproportionately apply to departments with higher acceptance rates (e.g., STEM fields). Female applicants may disproportionately apply to departments with lower acceptance rates (e.g., humanities).\n",
    "\n",
    "Thus, while female applicants may have higher acceptance rates **within each department**, the overall acceptance rate for women is lower due to the distribution of applications across departments.\n",
    "\n",
    "  Causal DAG:\n",
    "  ```\n",
    "  Applicant Gender --> Department Applied To --> Acceptance Rate\n",
    "  ```\n",
    "\n",
    "In this DAG:\n",
    "  - Gender influences which department is applied to.\n",
    "  - The department determines the acceptance rate.\n",
    "  - If not properly stratified by department, the observed gender disparity in acceptance rates can appear misleading.\n",
    "\n",
    "Simpson's Paradox illustrates that aggregating across groups without accounting for such confounders can lead to paradoxical conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Average Treatment Effect\n",
    "\n",
    "## (a): Why $\\tau_{ATT}$ is Preferred in Certain Cases\n",
    "\n",
    "The Average Treatment Effect on the Treated ($\\tau_{ATT}$) is preferred when some units are very unlikely to be treated because:\n",
    "\n",
    "1. **Rare Treatment Groups**: For certain subpopulations (e.g., older unmarried men), there might be insufficient data to accurately estimate the effect of the treatment due to sparse overlap between treated and untreated units.\n",
    "2. **Focus on Treated Population**: $\\tau_{ATT}$ focuses exclusively on the population that actually receives the treatment, providing more reliable insights into the treatment's effect on this specific group.\n",
    "3. **Captures Qualitative Goal**: While $\\tau_{ATT}$ does not represent the overall causal effect for the entire population, it still provides meaningful information about the treatment's impact on the treated group.\n",
    "\n",
    "\n",
    "## (b): Plug-In Estimator for $\\tau_{ATT}$\n",
    "\n",
    "To estimate $\\tau_{ATT}$ using a plug-in estimator:\n",
    "1. **Estimate Conditional Expectations**:\n",
    "   - Use a model (e.g., Random Forest) to estimate $\\mathbb{E}[Y | A = 1, X]$ and $\\mathbb{E}[Y | A = 0, X]$ for the treated group $A = 1$.\n",
    "2. **Compute Differences**:\n",
    "   - For each treated unit ($A = 1$), compute the difference $\\hat{\\tau}(X) = \\mathbb{E}[Y | A = 1, X] - \\mathbb{E}[Y | A = 0, X]$.\n",
    "3. **Average Over Treated Units**:\n",
    "   - Average the computed differences for all treated units to estimate $\\tau_{ATT}$.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\hat{\\tau}_{ATT} = \\frac{1}{n_1} \\sum_{i: A_i = 1} \\left( \\mathbb{E}[Y | A = 1, X_i] - \\mathbb{E}[Y | A = 0, X_i] \\right)\n",
    "$$\n",
    "where $n_1$ is the number of treated units ($A = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) and (d) Code Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def make_data_lalonde(df):\n",
    "    df_new = df.drop(['nodegree'], axis=1)\n",
    "    df_new['pos74'] = (df_new['RE74'] > 0).astype(int)\n",
    "    df_new['pos75'] = (df_new['RE75'] > 0).astype(int)\n",
    "    df_new['treatment'] = df_new['treatment'].astype(int)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "col_names = ['treatment', 'age', 'education', 'black',\n",
    "             'hispanic', 'married', 'nodegree', 'RE74', 'RE75', 'RE78']\n",
    "control = pd.read_csv('https://raw.githubusercontent.com/anishazaveri/austen_plots/master/data/imbens-raw/psid_controls.txt', header=None, sep=r\"\\s\\s\", names=col_names, engine='python')\n",
    "treatment = pd.read_csv('https://raw.githubusercontent.com/anishazaveri/austen_plots/master/data/imbens-raw/nswre74_treated.txt', header=None, sep=r\"\\s\\s\", names=col_names, engine='python')\n",
    "\n",
    "lalonde1 = pd.concat([control, treatment]).reset_index(drop=True)\n",
    "lalonde1 = make_data_lalonde(lalonde1)\n",
    "\n",
    "confounders = lalonde1.drop(columns=['RE78', 'treatment'])\n",
    "outcome = lalonde1['RE78']\n",
    "treatment = lalonde1['treatment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug-In ATT Estimate: 1085.3404537748204\n",
      "95% Confidence Interval (Plug-In): (654.0181089834856, 1532.9075526666634)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model functions\n",
    "def make_Q_model():\n",
    "    return RandomForestRegressor(random_state=42, n_estimators=500)\n",
    "\n",
    "def make_g_model():\n",
    "    return RandomForestClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "\n",
    "# Cross-fitting functions\n",
    "def treatment_k_fold_fit_and_predict(make_model, X, A, n_splits):\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "        X_train, A_train = X.iloc[train_index], A.iloc[train_index]\n",
    "        model = make_model()\n",
    "        model.fit(X_train, A_train)\n",
    "        predictions[test_index] = model.predict_proba(X.iloc[test_index])[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X, y, A, n_splits, output_type):\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment['A'] = A\n",
    "    X0, X1 = X_w_treatment.copy(), X_w_treatment.copy()\n",
    "    X0['A'], X1['A'] = 0, 1\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        model = make_model()\n",
    "        model.fit(X_w_treatment.iloc[train_index], y.iloc[train_index])\n",
    "        predictions0[test_index] = model.predict(X0.iloc[test_index])\n",
    "        predictions1[test_index] = model.predict(X1.iloc[test_index])\n",
    "    return predictions0, predictions1\n",
    "\n",
    "# Predict nuisance parameters\n",
    "g = treatment_k_fold_fit_and_predict(make_g_model, confounders, treatment, n_splits=10)\n",
    "Q0, Q1 = outcome_k_fold_fit_and_predict(make_Q_model, confounders, outcome, treatment, n_splits=10, output_type=\"continuous\")\n",
    "\n",
    "# Data preparation for ATT\n",
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "\n",
    "# Function for plug-in ATT estimation\n",
    "def plugin_att_estimator(Q1, Q0, A):\n",
    "    # Subset for treated group\n",
    "    treated_idx = A == 1\n",
    "    # Compute the ATT as the average difference for treated units\n",
    "    tau_att = np.mean(Q1[treated_idx] - Q0[treated_idx])\n",
    "    return tau_att\n",
    "\n",
    "# Calculate the plug-in ATT estimate\n",
    "tau_ATT_plugin = plugin_att_estimator(data_and_nuisance_estimates['Q1'],\n",
    "                                      data_and_nuisance_estimates['Q0'],\n",
    "                                      data_and_nuisance_estimates['A'])\n",
    "\n",
    "# Bootstrap confidence interval for plug-in estimator\n",
    "n_bootstrap = 1000\n",
    "bootstrap_estimates_plugin = []\n",
    "\n",
    "# Perform bootstrap\n",
    "for _ in range(n_bootstrap):\n",
    "    # Resample data with replacement\n",
    "    bootstrap_sample = resample(data_and_nuisance_estimates, replace=True, random_state=42 + _)\n",
    "    \n",
    "    # Extract Q1, Q0, A from the bootstrap sample\n",
    "    Q1_bootstrap = bootstrap_sample['Q1']\n",
    "    Q0_bootstrap = bootstrap_sample['Q0']\n",
    "    A_bootstrap = bootstrap_sample['A']\n",
    "    \n",
    "    # Estimate ATT using the plug-in estimator\n",
    "    tau_att_bootstrap = plugin_att_estimator(Q1_bootstrap, Q0_bootstrap, A_bootstrap)\n",
    "    bootstrap_estimates_plugin.append(tau_att_bootstrap)\n",
    "\n",
    "# Compute 95% confidence interval\n",
    "ci_lower_plugin = np.percentile(bootstrap_estimates_plugin, 2.5)\n",
    "ci_upper_plugin = np.percentile(bootstrap_estimates_plugin, 97.5)\n",
    "\n",
    "# Display results\n",
    "print(f\"Plug-In ATT Estimate: {tau_ATT_plugin}\")\n",
    "print(f\"95% Confidence Interval (Plug-In): ({ci_lower_plugin}, {ci_upper_plugin})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The plug-in estimator provides a simpler and computationally efficient method for estimating $\\tau_{ATT}$, yielding a narrower confidence interval and a slightly lower point estimate ($1085.34$) compared to the AIPTW estimator ($1300.98$). The plug-in approach avoids the sensitivity to extreme propensity scores seen in AIPTW, making it preferable in cases with severe overlap issues. However, the AIPTW estimator is doubly robust and more reliable in datasets with well-balanced treatment assignment, as it adjusts for propensity score imbalance. Overall, the plug-in estimator is ideal for scenarios prioritizing simplicity and lower variance, while AIPTW offers robustness at the cost of higher variance and potential instability in small or imbalanced samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Spurious features and Fairness\n",
    "\n",
    "## (a) Statistics and Fairness notion\n",
    "\n",
    "### **1. Demographic Parity (DP)**:\n",
    "$$\n",
    "\\text{DP} := \\text{average}(\\sigma(\\hat{f}(x_i)) \\,|\\, z_i=1) - \\text{average}(\\sigma(\\hat{f}(x_i)) \\,|\\, z_i=0)\n",
    "$$\n",
    "- **Criterion**: Demographic parity ensures that the predictions $\\hat{f}(X)$ are independent of the sensitive attribute $Z$. This means that $\\hat{f}(X) \\perp\\!\\!\\!\\perp Z$.\n",
    "- **Equal to 0 When**: DP equals 0 if the model $\\hat{f}(X)$ satisfies demographic parity, i.e., the average predicted probability of the outcome is the same across groups $Z=1$ and $Z=0$.\n",
    "\n",
    "### **2. Equalized Odds (EO)**:\n",
    "$$\n",
    "\\text{EO} := \\text{average}(\\sigma(\\hat{f}(x_i)) \\,|\\, z_i=1, y_i=1) - \\text{average}(\\sigma(\\hat{f}(x_i)) \\,|\\, z_i=0, y_i=1)\n",
    "$$\n",
    "- **Criterion**: Equalized odds ensures that the predictions $\\hat{f}(X)$ are independent of $Z$, conditional on the true outcome $Y$. This means $\\hat{f}(X) \\perp\\!\\!\\!\\perp Z \\,|\\, Y$.\n",
    "- **Equal to 0 When**: EO equals 0 if the model $\\hat{f}(X)$ satisfies equalized odds, i.e., the true positive rate and false positive rate are the same across groups $Z=1$ and $Z=0$.\n",
    "\n",
    "### **3. Predictive Parity (PP)**:\n",
    "$$\n",
    "\\text{PP} := \\text{average}(|y_i - \\sigma(\\hat{f}(x_i))| \\,|\\, z_i=1) - \\text{average}(|y_i - \\sigma(\\hat{f}(x_i))| \\,|\\, z_i=0)\n",
    "$$\n",
    "- **Criterion**: Predictive parity ensures that the sensitive attribute $Z$ is independent of the true outcome $Y$, conditional on the predicted probability $\\hat{f}(X)$. This means $Z \\perp\\!\\!\\!\\perp Y \\,|\\, \\hat{f}(X)$.\n",
    "- **Equal to 0 When**: PP equals 0 if the model $\\hat{f}(X)$ satisfies predictive parity, i.e., the residuals (absolute differences between true $Y$ and predicted $\\hat{f}(X)$) are the same across groups $Z=1$ and $Z=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Fairness through unawareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Test Set Accuracy: 0.84\n",
      "Demographic Parity (DP): -0.1697\n",
      "Equalized Odds (EO): -0.1911\n",
      "Predictive Parity (PP): -0.2439\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# The dataset contains:\n",
    "# - Features for predicting income (e.g., age, education, work hours, etc.)\n",
    "# - \"income\" column: target variable (Y), binary (0/1)\n",
    "# - \"Z\" column: sensitive attribute\n",
    "df = pd.read_csv('fairness_data.csv')\n",
    "\n",
    "# Separate features (X), target (Y), and sensitive attribute (Z)\n",
    "X = df.drop(columns=[\"income\", \"Z\"])  # Features: exclude \"income\" (Y) and \"Z\" (sensitive attribute)\n",
    "Y = df[\"income\"]  # Target variable: income (binary)\n",
    "Z = df[\"Z\"]  # Sensitive attribute: Z (binary)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, Z_train, Z_test = train_test_split(X, Y, Z, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use a Gradient Boosting Classifier\n",
    "clf_ftu = GradientBoostingClassifier(random_state=42)\n",
    "clf_ftu.fit(X_train, Y_train)\n",
    "\n",
    "# Predict both class labels and probabilities\n",
    "Y_pred = clf_ftu.predict(X_test)\n",
    "Y_pred_proba = clf_ftu.predict_proba(X_test)[:, 1]  # Predicted probabilities for the positive class\n",
    "\n",
    "# Calculate test-set accuracy\n",
    "test_accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Demographic Parity (DP): Difference in average predicted probabilities across sensitive groups\n",
    "dp = np.mean(Y_pred_proba[Z_test == 1]) - np.mean(Y_pred_proba[Z_test == 0])\n",
    "\n",
    "# Equalized Odds (EO): Difference in average predicted probabilities conditioned on Y=1 across sensitive groups\n",
    "eo = np.mean(Y_pred_proba[(Z_test == 1) & (Y_test == 1)]) - np.mean(Y_pred_proba[(Z_test == 0) & (Y_test == 1)])\n",
    "\n",
    "# Predictive Parity (PP): Difference in average residuals (true Y - predicted probabilities) across sensitive groups\n",
    "pp = np.mean(np.abs(Y_test[Z_test == 1] - Y_pred_proba[Z_test == 1])) - \\\n",
    "     np.mean(np.abs(Y_test[Z_test == 0] - Y_pred_proba[Z_test == 0]))\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Demographic Parity (DP): {dp:.4f}\")\n",
    "print(f\"Equalized Odds (EO): {eo:.4f}\")\n",
    "print(f\"Predictive Parity (PP): {pp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Marginalizing out the sensitive attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Marginalized Classifier:\n",
      "Test Set Accuracy: 0.83\n",
      "Demographic Parity (DP): -0.0038\n",
      "Equalized Odds (EO): 0.0565\n",
      "Predictive Parity (PP): -0.1551\n"
     ]
    }
   ],
   "source": [
    "clf_with_z = GradientBoostingClassifier(random_state=42)\n",
    "X_with_z_train = X_train.copy()\n",
    "X_with_z_train[\"Z\"] = Z_train  # Include the sensitive attribute\n",
    "clf_with_z.fit(X_with_z_train, Y_train)\n",
    "\n",
    "# Create a new feature set without the sensitive attribute for prediction\n",
    "X_without_z_test = X_test.copy()\n",
    "\n",
    "# Predict for Z=0 and Z=1, then average the predictions\n",
    "X_with_z_test_0 = X_without_z_test.copy()\n",
    "X_with_z_test_0[\"Z\"] = 0  # Set Z=0 for all test data\n",
    "Y_pred_proba_z0 = clf_with_z.predict_proba(X_with_z_test_0)[:, 1]  # Predicted probabilities for Z=0\n",
    "\n",
    "X_with_z_test_1 = X_without_z_test.copy()\n",
    "X_with_z_test_1[\"Z\"] = 1  # Set Z=1 for all test data\n",
    "Y_pred_proba_z1 = clf_with_z.predict_proba(X_with_z_test_1)[:, 1]  # Predicted probabilities for Z=1\n",
    "\n",
    "# Average the predictions for Z=0 and Z=1 to get marginalized predictions\n",
    "Y_pred_proba_marginalized = (Y_pred_proba_z0 + Y_pred_proba_z1) / 2\n",
    "Y_pred_marginalized = (Y_pred_proba_marginalized > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate test-set accuracy\n",
    "test_accuracy_marginalized = accuracy_score(Y_test, Y_pred_marginalized)\n",
    "\n",
    "# Calculate fairness statistics\n",
    "# Demographic Parity (DP)\n",
    "dp_marginalized = np.mean(Y_pred_proba_marginalized[Z_test == 1]) - np.mean(Y_pred_proba_marginalized[Z_test == 0])\n",
    "\n",
    "# Equalized Odds (EO)\n",
    "eo_marginalized = np.mean(Y_pred_proba_marginalized[(Z_test == 1) & (Y_test == 1)]) - \\\n",
    "                  np.mean(Y_pred_proba_marginalized[(Z_test == 0) & (Y_test == 1)])\n",
    "\n",
    "# Predictive Parity (PP)\n",
    "pp_marginalized = np.mean(np.abs(Y_test[Z_test == 1] - Y_pred_proba_marginalized[Z_test == 1])) - \\\n",
    "                  np.mean(np.abs(Y_test[Z_test == 0] - Y_pred_proba_marginalized[Z_test == 0]))\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"Results for Marginalized Classifier:\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy_marginalized:.2f}\")\n",
    "print(f\"Demographic Parity (DP): {dp_marginalized:.4f}\")\n",
    "print(f\"Equalized Odds (EO): {eo_marginalized:.4f}\")\n",
    "print(f\"Predictive Parity (PP): {pp_marginalized:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
